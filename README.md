# gemma-lora-jax-experiments 🧪

Ce dépôt contient des expériences simples et reproductibles de fine-tuning de modèles en utilisant JAX, Flax, et d'autres frameworks modernes.

## 📁 Structure du dépôt

- `gemma_finetuning/` : Fine-tuning d’un modèle Gemma avec des techniques classiques.
- `lora_finetuning/` : Fine-tuning de Gemma avec la méthode LoRA (Low-Rank Adaptation).
- `cnn_training/` : Entraînement d’un CNN custom pour la classification d’images avec JAX/Flax.

## 🔧 Requirements

```bash
pip install -r requirements.txt
