# gemma-lora-jax-experiments ğŸ§ª

Ce dÃ©pÃ´t contient des expÃ©riences simples et reproductibles de fine-tuning de modÃ¨les en utilisant JAX, Flax, et d'autres frameworks modernes.

## ğŸ“ Structure du dÃ©pÃ´t

- `gemma_finetuning/` : Fine-tuning dâ€™un modÃ¨le Gemma avec des techniques classiques.
- `lora_finetuning/` : Fine-tuning de Gemma avec la mÃ©thode LoRA (Low-Rank Adaptation).
- `cnn_training/` : EntraÃ®nement dâ€™un CNN custom pour la classification dâ€™images avec JAX/Flax.

## ğŸ”§ Requirements

```bash
pip install -r requirements.txt
